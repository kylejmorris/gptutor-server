{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Input custom variables here --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAYLIST_URL = \"https://www.youtube.com/playlist?list=PL9zq2zalZB1JRZsPtYeFVQAZkrDbZx3Qw\" # My Learning Playlist \n",
    "TEST_URL = \"https://www.youtube.com/watch?v=vaUy6zyJfwU\" # 1 minute video \n",
    "\n",
    "WHISPER_MODEL_SIZE = \"tiny\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- How to use --- \n",
    "\n",
    "Export notebook as `transcribe_youtube.py`, then import to use the key functions\n",
    "\n",
    "\n",
    "\n",
    "### example usage \n",
    "\n",
    "In your Python script/notebook: \n",
    "\n",
    "```\n",
    "from transcribe_youtube import get_transcription_from_youtube_url, get_transcriptions_from_youtube_playlist\n",
    "\n",
    "PLAYLIST_URL = \"https://www.youtube.com/playlist?list=PL9zq2zalZB1JRZsPtYeFVQAZkrDbZx3Qw\" # My Learning Playlist \n",
    "TEST_URL = \"https://www.youtube.com/watch?v=vaUy6zyJfwU\" # 1 minute video \n",
    "\n",
    "# get transcription for a single video \n",
    "single_video_transcription = get_transcription_from_youtube_url(TEST_URL)\n",
    "\n",
    "# get transcriptions for all videos in a playlist\n",
    "playlist_transcriptions = get_transcriptions_from_youtube_playlist(PLAYLIST_URL)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### return object\n",
    "Returns a dict with `title`, `url`, and `transcription`\n",
    "\n",
    "\n",
    "Example: \n",
    "```\n",
    "{'title': 'Deep Learning Maps Animal Movement',\n",
    " 'url': 'https://www.youtube.com/watch?v=vaUy6zyJfwU',\n",
    " 'transcription': ' We developed a new type of 3D deep learning approach that can take in normal color videos of behaving animals and behaving humans and then output the precise 3D locations of body landmarks so skeletal joints that you can track over time and thus provide a comprehensive description of how subjects are moving. This is a huge leap forward compared to a traditional motion capture system in which subjects need to wear highly invasive markers on the body. And then another big issue with motion captures that it requires that you have a clear line of sights to these markers and in a deep learning-based approach that we develop, we relieve this requirement.'}\n",
    " ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Run Notebook --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install replicate\n",
    "# !pip install pytube\n",
    "# !pip install flask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytube import YouTube, Playlist "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Replicate's Whisper API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "REPLICATE_API_TOKEN = os.getenv(\"REPLICATE_API_KEY\")\n",
    "REPLICATE_MODEL_VERSION = os.getenv(\"REPLICATE_MODEL_VERSION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a replicate client \n",
    "import replicate\n",
    "client = replicate.Client(api_token=REPLICATE_API_TOKEN)\n",
    "model = client.models.get(\"openai/whisper\")\n",
    "version = model.versions.get(REPLICATE_MODEL_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_url):\n",
    "    output = version.predict(\n",
    "        audio=audio_url,\n",
    "        language=\"en\", \n",
    "        model=WHISPER_MODEL_SIZE\n",
    "    )\n",
    "\n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mp3_url_from_youtube(youtube_url):\n",
    "    mp3 = YouTube(youtube_url).streams.filter(only_audio=True).first()\n",
    "    return mp3.url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main function \n",
    "import time \n",
    "\n",
    "def get_raw_transcription_from_youtube_url(youtube_url):\n",
    "    start_time = time.time()\n",
    "    mp3_url = get_mp3_url_from_youtube(youtube_url)\n",
    "    whisperresponse = transcribe_audio(mp3_url)\n",
    "    # print (\"Time taken to transcribe (sec): \", time.time() - start_time)\n",
    "    return whisperresponse['transcription']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcription_from_youtube_url(youtube_url):\n",
    "    yt_object = YouTube(youtube_url)\n",
    "    title = yt_object.title\n",
    "    transcription = get_raw_transcription_from_youtube_url(youtube_url)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": youtube_url,\n",
    "        \"transcription\": transcription\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING \n",
    "# # create an mp3 object from the youtube video\n",
    "\n",
    "# mp3 = YouTube(TEST_URL).streams.filter(only_audio=True).first()\n",
    "# print (mp3.url)\n",
    "\n",
    "# whisperresponse = transcribe_audio(mp3.url)\n",
    "# trans = whisperresponse['transcription']\n",
    "# trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans = get_transcription_from_yt_url(TEST_URL)\n",
    "# trans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run transcription on an entire YouTube Playlist "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### playlist helpers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_from_youtube_playlist(playlist_url):\n",
    "    \"\"\"Returns a list of video urls from a youtube playlist\"\"\"\n",
    "    playlist = Playlist(playlist_url)\n",
    "    return playlist.video_urls\n",
    "\n",
    "# print (get_urls_from_youtube_playlist(PLAYLIST_URL))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main function: youtube playlist -> dict of transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the urls from the playlist\n",
    "\n",
    "def get_transcriptions_from_youtube_playlist(playlist_url):\n",
    "    \"\"\"\n",
    "    Returns a list of dictionaries with the following keys:\n",
    "    - title\n",
    "    - url\n",
    "    - transcription\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Get list of individual video URLS\n",
    "    playlist_url_list = get_urls_from_youtube_playlist(PLAYLIST_URL)\n",
    "\n",
    "    # Get playlist title \n",
    "    playlist_title = Playlist(PLAYLIST_URL).title\n",
    "\n",
    "    # ========\n",
    "    # For each video, get the transcription\n",
    "    transcriptions = [] # list to be returned \n",
    "\n",
    "    for url in playlist_url_list:\n",
    "        transcriptions.append(\n",
    "            get_transcription_from_youtube_url(url)\n",
    "        )\n",
    "\n",
    "    # OPTIONAL: save to a csv file\n",
    "    df = pd.DataFrame(transcriptions)\n",
    "    df.to_csv(\"{playlist_title}_transcriptions.csv\", index=False)\n",
    "\n",
    "    # log the time taken, round to 2 decimal places\n",
    "    print (f\"\\n===\\nTime taken to transcribe Playlist '{playlist_title}' (sec): \\n\", round(time.time() - start_time, 2), \"\\n===\")\n",
    "\n",
    "    return transcriptions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Run tests ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Deep Learning Maps Animal Movement',\n",
       " 'url': 'https://www.youtube.com/watch?v=vaUy6zyJfwU',\n",
       " 'transcription': ' We developed a new type of 3D deep learning approach that can take in normal color videos of behaving animals and behaving humans and then output the precise 3D locations of body landmarks so skeletal joints that you can track over time and thus provide a comprehensive description of how subjects are moving. This is a huge leap forward compared to a traditional motion capture system in which subjects need to wear highly invasive markers on the body. And then another big issue with motion captures that it requires that you have a clear line of sights to these markers and in a deep learning-based approach that we develop, we relieve this requirement.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_video_transcription = get_transcription_from_youtube_url(TEST_URL)\n",
    "single_video_transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===\n",
      "Time taken to transcribe Playlist 'My Learning Playlist' (sec): \n",
      " 127.51 \n",
      "===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'Deep Learning Maps Animal Movement',\n",
       "  'url': 'https://www.youtube.com/watch?v=vaUy6zyJfwU',\n",
       "  'transcription': ' We developed a new type of 3D deep learning approach that can take in normal color videos of behaving animals and behaving humans and then output the precise 3D locations of body landmarks so skeletal joints that you can track over time and thus provide a comprehensive description of how subjects are moving. This is a huge leap forward compared to a traditional motion capture system in which subjects need to wear highly invasive markers on the body. And then another big issue with motion captures that it requires that you have a clear line of sights to these markers and in a deep learning-based approach that we develop, we relieve this requirement.'},\n",
       " {'title': \"First look: Stable Diffusion's Top 10 Datasets\",\n",
       "  'url': 'https://www.youtube.com/watch?v=CcKJD7PUaZg',\n",
       "  'transcription': \" I don't think I've ever done a one minute video before. We're wasting time. Stable diffusions top 10 datasets. I generated this image via stable diffusion, which is available for free online. Stable diffusion is one of the largest text image models in the world. It used four times as many images in training compared to open AR's Dolly 2. Just so reminder that all text image models only look at images during training. They do not store and they cannot reproduce the original images. Number one, Pinterest at 8.6% of stable diffusion's data set. Then WordPress. Find out America the marketplace. Once or three RF, the stock photo site. Shopify smug mug, square space. Daily mail the new site, Wikimedia, photo shelter, and two billion more images. I'd love to have you join me to learn more about these kind of things at the Memor.\"}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playlist_transcriptions = get_transcriptions_from_youtube_playlist(PLAYLIST_URL)\n",
    "playlist_transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d905c6a59c21f0f46be93fdc832728644d115a3fdfd57971d06d899b53e0576e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
